segment_select:
  # id: '<10000'
  has_data: 'true'
  maxgap_numsamples: '[-0.5, 0.5]'


# Advanced settings tuning the process routine:
advanced_settings:
  # Use parallel sub-processes to speed up the execution.
  multi_process: true
  # The number of sub-processes. If null, it is set as the the number of CPUs in the
  # system. This option is ignored if multi_process is false
  num_processes: null
  # Although each segment is processed one at a time, loading segments in chunks from the
  # database is faster: the number below defines the chunk size. If multi_process is true,
  # the chunk size also defines how many segments will be loaded in each python sub-process.
  # Increasing this number might speed up execution but increases the memory usage.
  # When null, the chunk size defaults to 1200 if the number N of
  # segments to be processed is > 1200, otherwise N/10.
  segments_chunksize: null
  # Optional arguments for the output writer. Ignored for CSV output, for HDF output see:
  # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.HDFStore.append.html
  # (the parameters 'append' and 'value' will be ignored, if given here)
  writer_options:
    chunksize: 10000
    # hdf needs a fixed length for all columns: for variable-length string columns,
    # you need to tell in advance how many bytes to allocate with 'min_itemsize'.
    # E.g., if you have two string columns 'col1' and 'col2' and you assume to store
    # at most 10 ASCII characters in 'col1' and 20 in 'col2', then:
    # min_itemsize:
    #   channel: 3
    #   ev_mty: 2
    #   st_net: 2
    #   st_name: 5
